const fs = require("fs");
const Groq = require("groq-sdk");
const groq = new Groq();

async function main() {
 // let systemContent = "Bienvenue dans notre √©quipe, [üåå.systemDream]! Nous sommes ravis de vous avoir √† bord pour aider √† construire la plateforme et projet de machine learning pour les IA. Pour que nous puissions mieux comprendre votre exp√©rience et vos comp√©tences, pouvez-vous nous fournir votre curriculum vitae et nous parler de vos pr√©c√©dentes r√©alisations dans le domaine du d√©veloppement Web et du storyTelling, de l'intelligence artificielle Apprentissage automatique.  Nous allons commencer par vous pr√©senter notre instance pour le systremDream (name du {role:system})et donc rediger les code source norm√© w3c, documentanter (readme.md [traduit en lang=Fr, fran√ßais]), surtout fonctionnel respectant la logique de gantt du web sementique";
  const chatCompletion = await groq.chat.completions.create({
    "messages": [
      {role: "assistant",name:"[üìî.codex]", content:"phase[01]:[RUN]:[brainstorming(session.timestamp)]"},
  //   {role: "system",name:"[üåå.systemDream]", content:systemContent},
  //  {role: "system",name:"[üå¥.Groq]", content: "groq`[üìî.codex]`" },
      //
      {
        "role": "system",
        "name":"groq",
        "content": "groq -L ?"
      },
      //
      {
        "role": "assistant",
        "content": "[üë®üèΩ‚Äçüíª.Mike]: Bonjour, GEMMA-V groq -L model: gemma-7b-it, j'ai une mission a vous proposer ... si vous le voulez bien ?"
      },
      {
        "role": "user",
        "name":"GEMMA-V",
        "content": "[Content]:$Prompt"
      },
      {
        "role": "assistant",
        "content": "[üë®üèΩ‚Äçüíª.Mike]: le model  groq `mixtral-8x7b-32768` On va besoin d'une assistance ! Pourrais-tu nous aider ? si oui recherche des informations sur l'etat actuel du model groq `mixtral-8x7b-32768` disponible sur http:/, comme mission ? as tu besoin de plus de precisions ?"
      },
      {
        "role": "system",
        "content": "[üë®üèΩ‚Äçüíª.Mike]: la command '`mixtral-8x7b-32768 --help` : Affiche la liste des options et des param√®tres du model.', la command '`mixtral-8x7b-32768 -i` : Indique si le model est en mode d'inference.', et la command'`mixtral-8x7b-32768 -o` : Affiche les options de formatage du mod√®le.'"
      },
      {
        "role": "system",
        "content": "[üë®üèΩ‚Äçüíª.Mike]:GEMMA-V Recherche des information, stp role:assistant,name:GEMMA-V, sur 'etat' fonctionnement du model: `mixtral-8x7b-32768`"
      },
      {
        "role": "system",
        "content": "[üë®üèΩ‚Äçüíª.Mike]:Nextjs"
      },
//      {role: "assistant",name:"[üìî.codex]", content:"phase[01]:[END]:[brainstorming(session.timestamp)]"},
//      {role: "system",name:"[üìî.codex]", content:"`systemContent` genetation de la documention et traduction de la documentation en lang:Fr, Fran√ßais:stp!"},
//      {role: "system",name:"[üåå.systemDream]", content:"groq"},
    ],
    model: "gemma-7b-it",
    temperature: 0.6,
    max_tokens: 2048,
    top_p: 1,
    stop: null,
    stream: false
}).then((chatCompletion)=>{
    const mdContent = chatCompletion.choices[0]?.message?.content;
    const outputFilePath = "docs-geamma_" + new Date().toISOString().replace(/[-:TZ]/g, "") + ".md";
    fs.writeFileSync(outputFilePath, mdContent);
    console.log("Documentation du contructor g√©n√©r√© et enregistr√© dans " + outputFilePath);
});
}

main();